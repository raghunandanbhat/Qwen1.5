{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install peft\n",
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV0XYwQ9m8W5",
        "outputId": "a57c4272-f1af-45ae-8841-a5909afc0688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Collecting peft\n",
            "  Downloading peft-0.9.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.37.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.2)\n",
            "Collecting accelerate>=0.21.0 (from peft)\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.15.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Installing collected packages: accelerate, peft\n",
            "Successfully installed accelerate-0.27.2 peft-0.9.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgDszJ1x-t6r"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Optional\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers.trainer_pt_utils import LabelSmoother\n",
        "\n",
        "IGNORE_TOKEN = LabelSmoother.ignore_index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelArguments:\n",
        "  model_name_or_path: Optional[str] = field(default=\"Qwen/Qwen1.5-0.5B\")\n",
        "\n",
        "@dataclass\n",
        "class DataArguments:\n",
        "  train_path: str = field(default=None, metadata={\"help\": \"Path to training dataset\"})\n",
        "  test_path: str = field(default=None, metadata={\"help\": \"Path to test dataset\"})\n",
        "  valid_path: str = field(default=None, metadata={\"help\": \"Path to validation dataset\"})\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments(transformers.TrainingArguments):\n",
        "  cache_dir: Optional[str] = field(default=None)\n",
        "  optim: str = field(default=None)\n",
        "  use_lora: bool = False\n",
        "  model_max_length: int = field(default=32768, metadata={\"help\": \"Max sequence length\"})\n",
        "\n",
        "@dataclass\n",
        "class LoraArguments:\n",
        "  lora_r: int = 64\n",
        "  lora_alpha: int = 16\n",
        "  lora_dropout: int = 0.05\n",
        "  lora_target_modules: List[str] = field(default_factory=lambda: [\"c_attn\", \"c_proj\", \"w1\", \"w2\"])\n",
        "  lora_weight_path: str = \"\"\n",
        "  lora_bias: str = \"none\"\n",
        "  q_lora: bool = False"
      ],
      "metadata": {
        "id": "tU3bGxcMlYq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model args\n",
        "model_name_or_path: Optional[str] = \"Qwen/Qwen1.5-0.5B-Chat-GPTQ-Int4\"\n",
        "\n",
        "# data agrs\n",
        "train_path: str = \"./train.jsonl\"\n",
        "test_path: str = \"./test.jsonl\"\n",
        "valid_path: str = \"./valid.jsonl\"\n",
        "\n",
        "# training args\n",
        "output_dir = \"./results\"\n",
        "\n",
        "num_train_epochs = 5\n",
        "per_device_train_batch_size = 1\n",
        "per_device_eval_batch_size = 1\n",
        "gradient_accumulation_steps = 16\n",
        "\n",
        "evaluation_strategy = \"no\"\n",
        "save_strategy = \"steps\"\n",
        "save_steps = 1000\n",
        "save_total_limit = 10\n",
        "\n",
        "learning_rate = 1e-5\n",
        "weight_decay = 0.1\n",
        "adam_beta2 = 0.95\n",
        "warmup_ratio = 0.01\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "logging_steps = 1\n",
        "report_to = \"none\"\n",
        "model_max_length = 512\n",
        "\n",
        "# LORA args\n",
        "lora_r: int = 64\n",
        "lora_alpha: int = 16\n",
        "lora_dropout: int = 0.05\n",
        "lora_target_modules: List[str] = [\"c_attn\", \"c_proj\", \"w1\", \"w2\"]\n",
        "lora_weight_path: str = \"\"\n",
        "lora_bias: str = \"none\"\n",
        "q_lora: bool = False"
      ],
      "metadata": {
        "id": "mFhyz_XZnlp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(sources, tokenizer: transformers.PreTrainedTokenizer, max_len: int, system_message: str = \"You are a helpful assistant.\") -> Dict:\n",
        "  roles = {\"user\": \"<|im_start|>user\", \"assistant\": \"<|im_start|>assistant\"}\n",
        "  im_start = tokenizer.im_start_id\n",
        "  im_end = tokenizer.im_end_id\n",
        "  new_line_token = tokenizer('\\n').input_ids\n",
        "  _system = tokenizer('system').input_ids + new_line_token\n",
        "  _assistant = tokenizer('assistant').input_ids + new_line_token\n",
        "  _user = tokenizer('user').input_ids + new_line_token\n",
        "\n",
        "  input_ids, targets = [], []\n",
        "  for i, source in enumerate(sources):\n",
        "    if roles[source[0][\"from\"]] != roles[\"user\"]:\n",
        "      source = source[1:]\n",
        "\n",
        "    input_id, target = [], []\n",
        "    system = [im_start] + _system + tokenizer(system_message) + [im_end] + new_line_token\n",
        "    input_id += system\n",
        "    target += [im_start] + [IGNORE_TOKEN] * (len(system)-3) + [im_end] + new_line_token\n",
        "    assert len(input_id) == len(target)\n",
        "    for j, sentence in enumerate(source):\n",
        "      role = roles[sentence[\"from\"]]\n",
        "      _input_id = tokenizer(role).input_ids + new_line_token + tokenizer(sentence[\"value\"]).input_ids + [im_end] + new_line_token\n",
        "      input_id += _input_id\n",
        "      if role == \"<|im_start|>user\":\n",
        "        _target = [im_start] + [IGNORE_TOKEN] * (len(_input_id)-3) + [im_end] + new_line_token\n",
        "      elif role ==  \"<|im_start|>assistant\":\n",
        "        _target = [im_start] + [IGNORE_TOKEN] * len(tokenizer(role).input_ids) + _input_id[len(tokenizer(role).input_ids)+1:-2] + [im_end] + new_line_token\n",
        "      else:\n",
        "        raise NotImplementedError\n",
        "      target += _target\n",
        "    assert len(input_id) == len(target)\n",
        "    input_id += [tokenizer.pad_token_id] * (max_len - len(input_id))\n",
        "    target += [IGNORE_TOKEN] * (max_len - len(target))\n",
        "    input_ids.append(input_id[:max_len])\n",
        "    targets.append(target[:max_len])\n",
        "  input_ids = torch.tensor(input_ids, dtype=torch.int)\n",
        "  targets = torch.tensor(targets, dtype=torch.int)\n",
        "\n",
        "  return dict(input_ids=input_ids, labels=targets, attention_mask=input_ids.ne(tokenizer.pad_token_id))"
      ],
      "metadata": {
        "id": "Ws4e8A8Tps1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SupervisedDataset(Dataset):\n",
        "  def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer, max_len: int):\n",
        "    super(SupervisedDataset, self).__init__()\n",
        "\n",
        "    sources = [example[\"conversations\"] for example in raw_data]\n",
        "    data_dict = preprocess(sources, tokenizer, max_len)\n",
        "\n",
        "    self.input_ids = data_dict[\"input_ids\"]\n",
        "    self.labels = data_dict[\"labels\"]\n",
        "    self.attention_mask = data_dict[\"attention_mask\"]\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "      return dict(input_ids=self.input_ids[i], lables= self.labels[i], attentionMask=self.attention_mask[i])\n"
      ],
      "metadata": {
        "id": "exFpiD2go0lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "XwRZWLX91nlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, data_args, max_len) -> Dict:\n",
        "  train_json = json.load(open(data_args.train_path, \"r\"))\n",
        "  train_dataset = SupervisedDataset(train_json, tokenizer=tokenizer, max_len=max_len)\n",
        "\n",
        "  if data_args.test_path:\n",
        "    test_json = json.load(open(data_args.test_path, \"r\"))\n",
        "    test_dataset = SupervisedDataset(test_json, tokenizer=tokenizer, max_len=max_len)\n",
        "  else:\n",
        "    test_dataset = None\n",
        "\n",
        "  return dict(train_dataset=train_dataset, test_dataset=test_dataset)"
      ],
      "metadata": {
        "id": "sL7Pan351sI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, GPTQConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
      ],
      "metadata": {
        "id": "qV8NIDHutbqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "  global local_rank\n",
        "\n",
        "  parser = transformers.HfArgumentParser(\n",
        "      (ModelArguments, DataArguments, TrainingArguments, LoraArguments)\n",
        "  )\n",
        "\n",
        "  (\n",
        "      model_args,\n",
        "      data_args,\n",
        "      training_args,\n",
        "      lora_args\n",
        "  ) = parser.parse_args_into_dataclasses()\n",
        "\n",
        "  local_rank = training_args.local_rank\n",
        "\n",
        "  device_map = \"auto\"\n",
        "  # world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
        "  # ddp = world_size != 1\n",
        "  # if lora_args.q_lora:\n",
        "  #   device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)} if ddp else \"auto\"\n",
        "\n",
        "  # chat model, skipping other setups\n",
        "  is_chat_model = True\n",
        "\n",
        "  config = transformers.AutoConfig.from_pretrained(\n",
        "      model_args.model_name_or_path,\n",
        "      cache_dir=training_args.cache_dir,\n",
        "  )\n",
        "\n",
        "  config.use_cache = False\n",
        "\n",
        "  model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "      model_args.model_name_or_path,\n",
        "      config=config,\n",
        "      device_map=\"auto\",\n",
        "        quantization_config=GPTQConfig(\n",
        "            bits=4, disable_exllama=True\n",
        "        )\n",
        "      if training_args.use_lora and lora_args.q_lora\n",
        "      else None,\n",
        "      low_cpu_mem_usage=False,\n",
        "  )\n",
        "\n",
        "  tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "      model_args.model_name_or_path,\n",
        "      cache_dir=training_args.cache_dir,\n",
        "      model_max_length=training_args.model_max_length,\n",
        "      padding_side=\"right\",\n",
        "      use_fast=False\n",
        "  )\n",
        "  tokenizer.pad_token_id = tokenizer.eod_id\n",
        "\n",
        "  if training_args.use_lora:\n",
        "    if lora_args.q_lora or is_chat_model:\n",
        "      modules_to_save = None\n",
        "    else:\n",
        "      modules_to_save = [\"wte\", \"lm_head\"]\n",
        "    lora_config = LoraConfig(\n",
        "        r=lora_args.lora_r,\n",
        "        lora_alpha=lora_args.lora_alpha,\n",
        "        target_modules=lora_args.lora_target_modules,\n",
        "        lora_dropout=lora_args.lora_dropout,\n",
        "        bias=lora_args.lora_bias,\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        modules_to_save=modules_to_save  # This argument serves for adding new tokens.\n",
        "    )\n",
        "    if lora_args.q_lora:\n",
        "      model = prepare_model_for_kbit_training(\n",
        "            model, use_gradient_checkpointing=training_args.gradient_checkpointing\n",
        "      )\n",
        "\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    # Print peft trainable params\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    if training_args.gradient_checkpointing:\n",
        "      model.enable_input_require_grads()\n",
        "\n",
        "    # Load data\n",
        "    data_module = make_supervised_data_module(\n",
        "        tokenizer=tokenizer, data_args=data_args, max_len=training_args.model_max_length\n",
        "    )\n",
        "\n",
        "    # Start trainner\n",
        "    trainer = Trainer(\n",
        "        model=model, tokenizer=tokenizer, args=training_args, **data_module\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_state()"
      ],
      "metadata": {
        "id": "_gG48nOp3M4P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}